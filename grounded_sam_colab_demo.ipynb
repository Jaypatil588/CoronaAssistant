{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaypatil588/CoronaAssistant/blob/master/grounded_sam_colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Grounded SAM Inpainting Demo](https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam_inpainting_demo.png)\n",
        "\n",
        "## Why this project?\n",
        "\n",
        "- [Segment Anything](https://github.com/facebookresearch/segment-anything) is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.\n",
        "- [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.\n",
        "- The combination of the two models enable **to detect and segment everything** with text inputs!\n",
        "\n"
      ],
      "metadata": {
        "id": "O49XQD8rwNUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "kAi4IkEfKi5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJqZHr3JTSB",
        "outputId": "f019a8b2-bb48-4b13-d2d6-6d51eb7b797d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Grounded-Segment-Anything'...\n",
            "remote: Enumerating objects: 1807, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -r requirements.txt\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything/segment_anything\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "MRkQUrtjKpbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "# Grounding DINO\n",
        "import GroundingDINO.groundingdino.datasets.transforms as T\n",
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
        "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "# segment anything\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# diffusers\n",
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "\n",
        "\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "6Osu6KERJeTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "IFnZ_nwcKuUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "004CGse3NRS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grounding DINO model"
      ],
      "metadata": {
        "id": "qzhOnA6ALQlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=device)\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "dPTgK04-J1U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "\n",
        "\n",
        "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)"
      ],
      "metadata": {
        "id": "73fWWbE0KBZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAM"
      ],
      "metadata": {
        "id": "uRsDao4wLV4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
        "\n",
        "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
      ],
      "metadata": {
        "id": "QNyrl-WMLVQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Diffusion (Inpainting)"
      ],
      "metadata": {
        "id": "ZiDBwfq0LeVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "KmSGdNxYLkGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "y86GkczAOAVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "def download_image(url, image_file_path):\n",
        "    r = requests.get(url, timeout=4.0)\n",
        "    if r.status_code != requests.codes.ok:\n",
        "        assert False, 'Status code error: {}.'.format(r.status_code)\n",
        "\n",
        "    with Image.open(BytesIO(r.content)) as im:\n",
        "        im.save(image_file_path)\n",
        "    print('Image downloaded from url: {} and saved to: {}.'.format(url, image_file_path))\n",
        "\n",
        "\n",
        "local_image_path = \"assets/inpaint_demo.jpg\"\n",
        "image_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "\n",
        "download_image(image_url, local_image_path)\n",
        "image_source, image = load_image(local_image_path)\n",
        "Image.fromarray(image_source)"
      ],
      "metadata": {
        "id": "dad-C9F_OWn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grounding DINO for detection"
      ],
      "metadata": {
        "id": "NaEIyVwjOCam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect object using grounding DINO\n",
        "def detect(image, text_prompt, model, box_threshold = 0.3, text_threshold = 0.25):\n",
        "  boxes, logits, phrases = predict(\n",
        "      model=model,\n",
        "      image=image,\n",
        "      caption=text_prompt,\n",
        "      box_threshold=box_threshold,\n",
        "      text_threshold=text_threshold\n",
        "  )\n",
        "\n",
        "  annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "  annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
        "  return annotated_frame, boxes"
      ],
      "metadata": {
        "id": "lm8fGM5XOkqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame, detected_boxes = detect(image, text_prompt=\"bench\", model=groundingdino_model)\n",
        "Image.fromarray(annotated_frame)"
      ],
      "metadata": {
        "id": "YVjdmTWOPxTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM for segmentation"
      ],
      "metadata": {
        "id": "6p7cHFDKQw5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment(image, sam_model, boxes):\n",
        "  sam_model.set_image(image)\n",
        "  H, W, _ = image.shape\n",
        "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
        "  masks, _, _ = sam_model.predict_torch(\n",
        "      point_coords = None,\n",
        "      point_labels = None,\n",
        "      boxes = transformed_boxes,\n",
        "      multimask_output = False,\n",
        "      )\n",
        "  return masks.cpu()\n",
        "\n",
        "\n",
        "def draw_mask(mask, image, random_color=True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "\n",
        "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
        "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
        "\n",
        "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))"
      ],
      "metadata": {
        "id": "k8-1lhHjQ2dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
        "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
        "Image.fromarray(annotated_frame_with_mask)"
      ],
      "metadata": {
        "id": "puUUEAAjSlug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion for inpainting"
      ],
      "metadata": {
        "id": "Ct786WclTVdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mask images\n",
        "mask = segmented_frame_masks[0][0].cpu().numpy()\n",
        "inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "image_source_pil = Image.fromarray(image_source)\n",
        "image_mask_pil = Image.fromarray(mask)\n",
        "inverted_image_mask_pil = Image.fromarray(inverted_mask)\n",
        "\n",
        "\n",
        "display(*[image_source_pil, image_mask_pil, inverted_image_mask_pil])"
      ],
      "metadata": {
        "id": "wmBncdm8TVF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(image, mask, prompt, negative_prompt, pipe, seed):\n",
        "  # resize for inpainting\n",
        "  w, h = image.size\n",
        "  in_image = image.resize((512, 512))\n",
        "  in_mask = mask.resize((512, 512))\n",
        "\n",
        "  generator = torch.Generator(device).manual_seed(seed)\n",
        "\n",
        "  result = pipe(image=in_image, mask_image=in_mask, prompt=prompt, negative_prompt=negative_prompt, generator=generator)\n",
        "  result = result.images[0]\n",
        "\n",
        "  return result.resize((w, h))"
      ],
      "metadata": {
        "id": "VWKOb_ZWWx2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"A sofa, high quality, detailed, cyberpunk, futuristic, with a lot of details, and a lot of colors.\"\n",
        "negative_prompt=\"low resolution, ugly\"\n",
        "seed = 32 # for reproducibility\n",
        "\n",
        "generated_image = generate_image(image=image_source_pil, mask=image_mask_pil, prompt=prompt, negative_prompt=negative_prompt, pipe=sd_pipe, seed=seed)\n",
        "generated_image"
      ],
      "metadata": {
        "id": "IkRMVCOkYU68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"a beach with turquoise water, sand, and coconuts\"\n",
        "negative_prompt=\"people, low resolution, ugly\"\n",
        "seed = 32 # for reproducibility\n",
        "\n",
        "generated_image = generate_image(image_source_pil, inverted_image_mask_pil, prompt, negative_prompt, sd_pipe, seed)\n",
        "generated_image"
      ],
      "metadata": {
        "id": "s3LxZK1EY69d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFPPGHaOuTJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}